{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7124e50c-44d4-4611-80c6-64ec32f38c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is F0FA-76E0\n",
      "\n",
      " Directory of C:\\Users\\semih\\Desktop\\bulldozer\\NTLFlowLyzer\n",
      "\n",
      "10.02.2026  15:24    <DIR>          .\n",
      "10.02.2026  14:41    <DIR>          ..\n",
      "20.11.2025  20:59                50 .gitignore\n",
      "10.02.2026  14:21    <DIR>          .ipynb_checkpoints\n",
      "20.11.2025  20:59           127.230 Architecture.svg\n",
      "20.11.2025  20:59            22.504 bccc.jpg\n",
      "09.02.2026  19:54        28.595.372 capWIN-J6GMIG1DQE5-172.31.65.116\n",
      "20.11.2025  20:59    <DIR>          docs\n",
      "20.11.2025  20:59            35.823 LICENSE\n",
      "20.11.2025  21:30    <DIR>          NTLFlowLyzer\n",
      "10.02.2026  14:23         1.753.395 output.csv\n",
      "10.02.2026  15:24           495.897 random_forest_model1.joblib\n",
      "20.11.2025  20:59            60.470 README.md\n",
      "20.11.2025  20:59                29 requirements.txt\n",
      "10.02.2026  14:23            12.406 run_config.json\n",
      "20.11.2025  20:59               829 setup.py\n",
      "09.02.2026  19:53        14.060.513 test.pcap\n",
      "              12 File(s)     45.164.518 bytes\n",
      "               5 Dir(s)  25.499.164.672 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83baccbb-7982-492d-9402-6b583517f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Girdi DosyasÄ±: C:/Users/semih/Desktop/bulldozer/NTLFlowLyzer/test3.pcap\n",
      "ðŸ“ Ã‡Ä±ktÄ± DosyasÄ±: C:/Users/semih/Desktop/bulldozer/NTLFlowLyzer/output3.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# 1. Dosya YollarÄ±nÄ± Otomatik Buluyoruz (Hata ihtimalini sÄ±fÄ±rlar)\n",
    "current_dir = os.getcwd().replace(\"\\\\\", \"/\")  # Windows slash dÃ¼zeltmesi\n",
    "pcap_path = f\"{current_dir}/test3.pcap\"\n",
    "output_path = f\"{current_dir}/output3.csv\"\n",
    "\n",
    "print(f\"ðŸ“ Girdi DosyasÄ±: {pcap_path}\")\n",
    "print(f\"ðŸ“ Ã‡Ä±ktÄ± DosyasÄ±: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62036cdd-454a-4e1b-a998-af38f03a0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    \"pcap_file_address\": pcap_path,        \n",
    "    \"output_file_address\": output_path,    \n",
    "    \"number_of_threads\": 4,\n",
    "    \"feature_extractor_min_flows\": 5,     \n",
    "    \"writer_min_rows\": 1,                 \n",
    "    \"read_packets_count_value_log_info\": 1000,\n",
    "    \"check_flows_ending_min_flows\": 10,\n",
    "    \"capturer_updating_flows_min_value\": 50,\n",
    "    \"max_flow_duration\": 10000,\n",
    "    \"activity_timeout\": 5,\n",
    "    \"floating_point_unit\": \".4f\",\n",
    "    \"max_rows_number\": 1000000,\n",
    "    \"features_ignore_list\":['flow_id', 'timestamp', 'src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', 'duration', 'packets_count', 'fwd_packets_count', 'bwd_packets_count', 'payload_bytes_variance', 'payload_bytes_median', 'payload_bytes_skewness', 'payload_bytes_cov', 'payload_bytes_mode', 'fwd_payload_bytes_min', 'fwd_payload_bytes_median', 'fwd_payload_bytes_skewness', 'fwd_payload_bytes_cov', 'fwd_payload_bytes_mode', 'bwd_payload_bytes_min', 'bwd_payload_bytes_median', 'bwd_payload_bytes_skewness', 'bwd_payload_bytes_cov', 'bwd_payload_bytes_mode', 'total_header_bytes', 'max_header_bytes', 'min_header_bytes', 'mean_header_bytes', 'std_header_bytes', 'median_header_bytes', 'skewness_header_bytes', 'cov_header_bytes', 'mode_header_bytes', 'variance_header_bytes', 'fwd_total_header_bytes', 'fwd_max_header_bytes', 'fwd_min_header_bytes', 'fwd_mean_header_bytes', 'fwd_std_header_bytes', 'fwd_median_header_bytes', 'fwd_skewness_header_bytes', 'fwd_cov_header_bytes', 'fwd_mode_header_bytes', 'fwd_variance_header_bytes', 'bwd_total_header_bytes', 'bwd_max_header_bytes', 'bwd_min_header_bytes', 'bwd_mean_header_bytes', 'bwd_std_header_bytes', 'bwd_median_header_bytes', 'bwd_skewness_header_bytes', 'bwd_cov_header_bytes', 'bwd_mode_header_bytes', 'bwd_variance_header_bytes', 'fwd_segment_size_max', 'fwd_segment_size_min', 'fwd_segment_size_std', 'fwd_segment_size_variance', 'fwd_segment_size_median', 'fwd_segment_size_skewness', 'fwd_segment_size_cov', 'fwd_segment_size_mode', 'bwd_segment_size_max', 'bwd_segment_size_min', 'bwd_segment_size_std', 'bwd_segment_size_variance', 'bwd_segment_size_median', 'bwd_segment_size_skewness', 'bwd_segment_size_cov', 'bwd_segment_size_mode','segment_size_max', 'segment_size_min', 'segment_size_std', 'segment_size_variance', 'segment_size_median', 'segment_size_skewness', 'segment_size_cov', 'segment_size_mode', 'fwd_init_win_bytes', 'bwd_init_win_bytes', 'active_min', 'active_max', 'active_mean', 'active_std', 'active_median', 'active_skewness', 'active_cov', 'active_mode', 'active_variance', 'idle_min', 'idle_max', 'idle_mean', 'idle_std', 'idle_median', 'idle_skewness', 'idle_cov', 'idle_mode', 'idle_variance', 'bytes_rate', 'fwd_bytes_rate', 'bwd_bytes_rate', 'packets_rate', 'bwd_packets_rate', 'fwd_packets_rate', 'down_up_rate', 'avg_fwd_bytes_per_bulk', 'avg_fwd_packets_per_bulk', 'avg_fwd_bulk_rate', 'avg_bwd_bytes_per_bulk', 'avg_bwd_packets_bulk_rate', 'avg_bwd_bulk_rate', 'fwd_bulk_state_count', 'fwd_bulk_total_size', 'fwd_bulk_per_packet', 'fwd_bulk_duration', 'bwd_bulk_state_count', 'bwd_bulk_total_size', 'bwd_bulk_per_packet', 'bwd_bulk_duration', 'fin_flag_counts', 'psh_flag_counts', 'urg_flag_counts', 'ece_flag_counts', 'syn_flag_counts', 'ack_flag_counts', 'cwr_flag_counts', 'rst_flag_counts', 'fwd_fin_flag_counts', 'fwd_psh_flag_counts', 'fwd_urg_flag_counts', 'fwd_ece_flag_counts', 'fwd_syn_flag_counts', 'fwd_ack_flag_counts', 'fwd_cwr_flag_counts', 'fwd_rst_flag_counts', 'bwd_fin_flag_counts', 'bwd_psh_flag_counts', 'bwd_urg_flag_counts', 'bwd_ece_flag_counts', 'bwd_syn_flag_counts', 'bwd_ack_flag_counts', 'bwd_cwr_flag_counts', 'bwd_rst_flag_counts', 'fin_flag_percentage_in_total', 'psh_flag_percentage_in_total', 'urg_flag_percentage_in_total', 'ece_flag_percentage_in_total', 'syn_flag_percentage_in_total', 'ack_flag_percentage_in_total', 'cwr_flag_percentage_in_total', 'rst_flag_percentage_in_total', 'fwd_fin_flag_percentage_in_total', 'fwd_psh_flag_percentage_in_total', 'fwd_urg_flag_percentage_in_total', 'fwd_ece_flag_percentage_in_total', 'fwd_syn_flag_percentage_in_total', 'fwd_ack_flag_percentage_in_total', 'fwd_cwr_flag_percentage_in_total', 'fwd_rst_flag_percentage_in_total', 'bwd_fin_flag_percentage_in_total', 'bwd_psh_flag_percentage_in_total', 'bwd_urg_flag_percentage_in_total', 'bwd_ece_flag_percentage_in_total', 'bwd_syn_flag_percentage_in_total', 'bwd_ack_flag_percentage_in_total', 'bwd_cwr_flag_percentage_in_total', 'bwd_rst_flag_percentage_in_total', 'fwd_fin_flag_percentage_in_fwd_packets', 'fwd_psh_flag_percentage_in_fwd_packets', 'fwd_urg_flag_percentage_in_fwd_packets', 'fwd_ece_flag_percentage_in_fwd_packets', 'fwd_syn_flag_percentage_in_fwd_packets', 'fwd_ack_flag_percentage_in_fwd_packets', 'fwd_cwr_flag_percentage_in_fwd_packets', 'fwd_rst_flag_percentage_in_fwd_packets', 'bwd_fin_flag_percentage_in_bwd_packets', 'bwd_psh_flag_percentage_in_bwd_packets', 'bwd_urg_flag_percentage_in_bwd_packets', 'bwd_ece_flag_percentage_in_bwd_packets', 'bwd_syn_flag_percentage_in_bwd_packets', 'bwd_ack_flag_percentage_in_bwd_packets', 'bwd_cwr_flag_percentage_in_bwd_packets', 'bwd_rst_flag_percentage_in_bwd_packets', 'packets_IAT_mean', 'packet_IAT_std', 'packet_IAT_max', 'packet_IAT_min', 'packet_IAT_total', 'packets_IAT_median', 'packets_IAT_skewness', 'packets_IAT_cov', 'packets_IAT_mode', 'packets_IAT_variance', 'fwd_packets_IAT_mean', 'fwd_packets_IAT_std', 'fwd_packets_IAT_max', 'fwd_packets_IAT_min', 'fwd_packets_IAT_total', 'fwd_packets_IAT_median', 'fwd_packets_IAT_skewness', 'fwd_packets_IAT_cov', 'fwd_packets_IAT_mode', 'fwd_packets_IAT_variance', 'bwd_packets_IAT_mean', 'bwd_packets_IAT_std', 'bwd_packets_IAT_max', 'bwd_packets_IAT_min', 'bwd_packets_IAT_total', 'bwd_packets_IAT_median', 'bwd_packets_IAT_skewness', 'bwd_packets_IAT_cov', 'bwd_packets_IAT_mode', 'bwd_packets_IAT_variance', 'subflow_fwd_packets', 'subflow_bwd_packets', 'delta_start', 'handshake_duration', 'handshake_state', 'min_bwd_packets_delta_time', 'max_bwd_packets_delta_time', 'mean_packets_delta_time', 'mode_packets_delta_time', 'variance_packets_delta_time', 'std_packets_delta_time', 'median_packets_delta_time', 'skewness_packets_delta_time', 'cov_packets_delta_time', 'mean_bwd_packets_delta_time', 'mode_bwd_packets_delta_time', 'variance_bwd_packets_delta_time', 'std_bwd_packets_delta_time', 'median_bwd_packets_delta_time', 'skewness_bwd_packets_delta_time', 'cov_bwd_packets_delta_time', 'min_fwd_packets_delta_time', 'max_fwd_packets_delta_time', 'mean_fwd_packets_delta_time', 'mode_fwd_packets_delta_time', 'variance_fwd_packets_delta_time', 'std_fwd_packets_delta_time', 'median_fwd_packets_delta_time', 'skewness_fwd_packets_delta_time', 'cov_fwd_packets_delta_time', 'min_packets_delta_len', 'max_packets_delta_len', 'mean_packets_delta_len', 'mode_packets_delta_len', 'variance_packets_delta_len', 'std_packets_delta_len', 'median_packets_delta_len', 'skewness_packets_delta_len', 'cov_packets_delta_len', 'min_bwd_packets_delta_len', 'max_bwd_packets_delta_len', 'mean_bwd_packets_delta_len', 'mode_bwd_packets_delta_len', 'variance_bwd_packets_delta_len', 'std_bwd_packets_delta_len', 'median_bwd_packets_delta_len', 'skewness_bwd_packets_delta_len', 'cov_bwd_packets_delta_len', 'min_fwd_packets_delta_len', 'max_fwd_packets_delta_len', 'mean_fwd_packets_delta_len', 'mode_fwd_packets_delta_len', 'variance_fwd_packets_delta_len', 'std_fwd_packets_delta_len', 'median_fwd_packets_delta_len', 'skewness_fwd_packets_delta_len', 'cov_fwd_packets_delta_len', 'min_header_bytes_delta_len', 'max_header_bytes_delta_len', 'mean_header_bytes_delta_len', 'mode_header_bytes_delta_len', 'variance_header_bytes_delta_len', 'std_header_bytes_delta_len', 'median_header_bytes_delta_len', 'skewness_header_bytes_delta_len', 'cov_header_bytes_delta_len', 'min_bwd_header_bytes_delta_len', 'max_bwd_header_bytes_delta_len', 'mean_bwd_header_bytes_delta_len', 'mode_bwd_header_bytes_delta_len', 'variance_bwd_header_bytes_delta_len', 'std_bwd_header_bytes_delta_len', 'median_bwd_header_bytes_delta_len', 'skewness_bwd_header_bytes_delta_len', 'cov_bwd_header_bytes_delta_len', 'min_fwd_header_bytes_delta_len', 'max_fwd_header_bytes_delta_len', 'mean_fwd_header_bytes_delta_len', 'mode_fwd_header_bytes_delta_len', 'variance_fwd_header_bytes_delta_len', 'std_fwd_header_bytes_delta_len', 'median_fwd_header_bytes_delta_len', 'skewness_fwd_header_bytes_delta_len', 'cov_fwd_header_bytes_delta_len', 'min_payload_bytes_delta_len', 'max_payload_bytes_delta_len', 'mean_payload_bytes_delta_len', 'mode_payload_bytes_delta_len', 'variance_payload_bytes_delta_len', 'std_payload_bytes_delta_len', 'median_payload_bytes_delta_len', 'skewness_payload_bytes_delta_len', 'cov_payload_bytes_delta_len', 'min_bwd_payload_bytes_delta_len', 'max_bwd_payload_bytes_delta_len', 'mean_bwd_payload_bytes_delta_len', 'mode_bwd_payload_bytes_delta_len', 'variance_bwd_payload_bytes_delta_len', 'std_bwd_payload_bytes_delta_len', 'median_bwd_payload_bytes_delta_len', 'skewness_bwd_payload_bytes_delta_len', 'cov_bwd_payload_bytes_delta_len', 'min_fwd_payload_bytes_delta_len', 'max_fwd_payload_bytes_delta_len', 'mean_fwd_payload_bytes_delta_len', 'mode_fwd_payload_bytes_delta_len', 'variance_fwd_payload_bytes_delta_len', 'std_fwd_payload_bytes_delta_len', 'median_fwd_payload_bytes_delta_len', 'skewness_fwd_payload_bytes_delta_len', 'cov_fwd_payload_bytes_delta_len', 'label']\n",
    "   \n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e97a6fa7-0070-4f21-abef-2ac411690e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_name = \"run_config.json\"\n",
    "with open(json_name, 'w') as f:\n",
    "    json.dump(config_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d88eec05-3980-44cb-848e-1b233b35d16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You initiated NTLFlowLyzer!\n",
      ">> Analyzing the C:/Users/semih/Desktop/bulldozer/NTLFlowLyzer/test3.pcap ...\n",
      ">> The input PCAP file contains 317430 packets.\n",
      ">> Extracting features of 67 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 64 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 61 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 66 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 61 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 61 number of flows...\n",
      ">> Extracting features of 74 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 71 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 66 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 61 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 71 number of flows...\n",
      ">> Extracting features of 82 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 69 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 83 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 67 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 62 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 62 number of flows...\n",
      ">> Extracting features of 67 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 66 number of flows...\n",
      ">> Extracting features of 81 number of flows...\n",
      ">> Extracting features of 71 number of flows...\n",
      ">> Extracting features of 65 number of flows...\n",
      ">> Extracting features of 65 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 88 number of flows...\n",
      ">> Extracting features of 69 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 63 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 69 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 62 number of flows...\n",
      ">> Extracting features of 68 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 63 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 67 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 78 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 63 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 71 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 67 number of flows...\n",
      ">> Extracting features of 65 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 64 number of flows...\n",
      ">> Extracting features of 64 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 64 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Writing 67 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 64 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 61 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 66 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 111 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 61 flows with extracted features...\n",
      ">> Writing 74 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 71 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 66 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 61 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 71 flows with extracted features...\n",
      ">> Writing 82 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 69 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 60 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 83 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 67 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 60 flows with extracted features...\n",
      ">> Writing 62 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 60 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 62 flows with extracted features...\n",
      ">> Writing 67 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 66 flows with extracted features...\n",
      ">> Writing 81 flows with extracted features...\n",
      ">> Writing 71 flows with extracted features...\n",
      ">> Writing 65 flows with extracted features...\n",
      ">> Writing 65 flows with extracted features...\n",
      ">> Writing 60 flows with extracted features...\n",
      ">> Writing 88 flows with extracted features...\n",
      ">> Writing 69 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 63 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Writing 127 flows with extracted features...\n",
      ">> Writing 62 flows with extracted features...\n",
      ">> Writing 68 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 63 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Writing 124 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 60 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 78 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 63 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 71 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 105 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 60 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 67 flows with extracted features...\n",
      ">> Writing 65 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 64 flows with extracted features...\n",
      ">> Writing 64 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 64 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 61 flows with extracted features...\n",
      ">> Writing 60 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Writing 87 flows with extracted features...\n",
      ">> Writing 69 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 63 flows with extracted features...\n",
      ">> Writing 63 flows with extracted features...>> Parser has started...\n",
      ">> Analyzing C:/Users/semih/Desktop/bulldozer/NTLFlowLyzer/test3.pcap\n",
      "==================================================\n",
      "Number and percentage of IP packets:\n",
      "  Total IP packets: 314434\n",
      "  Percentage of IP packets: 99.06%\n",
      "\n",
      "Number and percentage of TCP packets:\n",
      "  Total TCP packets: 301499\n",
      "  Percentage of TCP packets: 94.98%\n",
      "\n",
      "Number and percentage of UDP packets:\n",
      "  Total UDP packets: 12825\n",
      "  Percentage of UDP packets: 4.04%\n",
      "\n",
      "Top 10 Application Layer Protocols:\n",
      "  Port 51353 (Unknown): 109379 packets, 34.46%\n",
      "  Port 443 (HTTPS): 37059 packets, 11.67%\n",
      "  Port 50642 (Unknown): 30418 packets, 9.58%\n",
      "  Port 3389 (RDP): 18415 packets, 5.80%\n",
      "  Port 80 (HTTP): 10134 packets, 3.19%\n",
      "  Port 445 (Unknown): 10005 packets, 3.15%\n",
      "  Port 50294 (Unknown): 7889 packets, 2.49%\n",
      "  Port 53 (DNS): 6293 packets, 1.98%\n",
      "  Port 51142 (Unknown): 3577 packets, 1.13%\n",
      "  Port 49363 (Unknown): 3279 packets, 1.03%\n",
      "==================================================\n",
      ">> 1000 number of packets has been processed...\n",
      ">> 2000 number of packets has been processed...\n",
      ">> 3000 number of packets has been processed...\n",
      ">> 4000 number of packets has been processed...\n",
      ">> 5000 number of packets has been processed...\n",
      ">> 6000 number of packets has been processed...\n",
      ">> 7000 number of packets has been processed...\n",
      ">> 8000 number of packets has been processed...\n",
      ">> 10000 number of packets has been processed...\n",
      ">> 11000 number of packets has been processed...\n",
      ">> 12000 number of packets has been processed...\n",
      ">> 13000 number of packets has been processed...\n",
      ">> 14000 number of packets has been processed...\n",
      ">> 15000 number of packets has been processed...\n",
      ">> 16000 number of packets has been processed...\n",
      ">> 17000 number of packets has been processed...\n",
      ">> 18000 number of packets has been processed...\n",
      ">> 19000 number of packets has been processed...\n",
      ">> 20000 number of packets has been processed...\n",
      ">> 21000 number of packets has been processed...\n",
      ">> 22000 number of packets has been processed...\n",
      ">> 23000 number of packets has been processed...\n",
      ">> 24000 number of packets has been processed...\n",
      ">> 25000 number of packets has been processed...\n",
      ">> 26000 number of packets has been processed...\n",
      ">> 27000 number of packets has been processed...\n",
      ">> 28000 number of packets has been processed...\n",
      ">> 29000 number of packets has been processed...\n",
      ">> 30000 number of packets has been processed...\n",
      ">> 31000 number of packets has been processed...\n",
      ">> 32000 number of packets has been processed...\n",
      ">> 33000 number of packets has been processed...\n",
      ">> 34000 number of packets has been processed...\n",
      ">> 35000 number of packets has been processed...\n",
      ">> 36000 number of packets has been processed...\n",
      ">> 37000 number of packets has been processed...\n",
      ">> 38000 number of packets has been processed...\n",
      ">> 39000 number of packets has been processed...\n",
      ">> 40000 number of packets has been processed...\n",
      ">> 41000 number of packets has been processed...\n",
      ">> 42000 number of packets has been processed...\n",
      ">> 43000 number of packets has been processed...\n",
      ">> 44000 number of packets has been processed...\n",
      ">> 46000 number of packets has been processed...\n",
      ">> 47000 number of packets has been processed...\n",
      ">> 48000 number of packets has been processed...\n",
      ">> 49000 number of packets has been processed...\n",
      ">> 50000 number of packets has been processed...\n",
      ">> 51000 number of packets has been processed...\n",
      ">> 52000 number of packets has been processed...\n",
      ">> 53000 number of packets has been processed...\n",
      ">> 54000 number of packets has been processed...\n",
      ">> 56000 number of packets has been processed...\n",
      ">> 58000 number of packets has been processed...\n",
      ">> 59000 number of packets has been processed...\n",
      ">> 60000 number of packets has been processed...\n",
      ">> 61000 number of packets has been processed...\n",
      ">> 62000 number of packets has been processed...\n",
      ">> 63000 number of packets has been processed...\n",
      ">> 64000 number of packets has been processed...\n",
      ">> 65000 number of packets has been processed...\n",
      ">> 66000 number of packets has been processed...\n",
      ">> 67000 number of packets has been processed...\n",
      ">> 68000 number of packets has been processed...\n",
      ">> 69000 number of packets has been processed...\n",
      ">> 70000 number of packets has been processed...\n",
      ">> 71000 number of packets has been processed...\n",
      ">> 72000 number of packets has been processed...\n",
      ">> 73000 number of packets has been processed...\n",
      ">> 74000 number of packets has been processed...\n",
      ">> 75000 number of packets has been processed...\n",
      ">> 76000 number of packets has been processed...\n",
      ">> 77000 number of packets has been processed...\n",
      ">> 78000 number of packets has been processed...\n",
      ">> 79000 number of packets has been processed...\n",
      ">> 80000 number of packets has been processed...\n",
      ">> 81000 number of packets has been processed...\n",
      ">> 82000 number of packets has been processed...\n",
      ">> 83000 number of packets has been processed...\n",
      ">> 84000 number of packets has been processed...\n",
      ">> 85000 number of packets has been processed...\n",
      ">> 86000 number of packets has been processed...\n",
      ">> 87000 number of packets has been processed...\n",
      ">> 88000 number of packets has been processed...\n",
      ">> 89000 number of packets has been processed...\n",
      ">> 90000 number of packets has been processed...\n",
      ">> 91000 number of packets has been processed...\n",
      ">> 92000 number of packets has been processed...\n",
      ">> 93000 number of packets has been processed...\n",
      ">> 94000 number of packets has been processed...\n",
      ">> 95000 number of packets has been processed...\n",
      ">> 96000 number of packets has been processed...\n",
      ">> 97000 number of packets has been processed...\n",
      ">> 98000 number of packets has been processed...\n",
      ">> 99000 number of packets has been processed...\n",
      ">> 100000 number of packets has been processed...\n",
      ">> 101000 number of packets has been processed...\n",
      ">> 102000 number of packets has been processed...\n",
      ">> 103000 number of packets has been processed...\n",
      ">> 104000 number of packets has been processed...\n",
      ">> 105000 number of packets has been processed...\n",
      ">> 106000 number of packets has been processed...\n",
      ">> 107000 number of packets has been processed...\n",
      ">> 108000 number of packets has been processed...\n",
      ">> 109000 number of packets has been processed...\n",
      ">> 110000 number of packets has been processed...\n",
      ">> 111000 number of packets has been processed...\n",
      ">> 112000 number of packets has been processed...\n",
      ">> 113000 number of packets has been processed...\n",
      ">> 114000 number of packets has been processed...\n",
      ">> 115000 number of packets has been processed...\n",
      ">> 116000 number of packets has been processed...\n",
      ">> 117000 number of packets has been processed...\n",
      ">> 118000 number of packets has been processed...\n",
      ">> 119000 number of packets has been processed...\n",
      ">> 120000 number of packets has been processed...\n",
      ">> 121000 number of packets has been processed...\n",
      ">> 122000 number of packets has been processed...\n",
      ">> 123000 number of packets has been processed...\n",
      ">> 124000 number of packets has been processed...\n",
      ">> 125000 number of packets has been processed...\n",
      ">> 126000 number of packets has been processed...\n",
      ">> 127000 number of packets has been processed...\n",
      ">> 128000 number of packets has been processed...\n",
      ">> 129000 number of packets has been processed...\n",
      ">> 130000 number of packets has been processed...\n",
      ">> 131000 number of packets has been processed...\n",
      ">> 132000 number of packets has been processed...\n",
      ">> 133000 number of packets has been processed...\n",
      ">> 135000 number of packets has been processed...\n",
      ">> 136000 number of packets has been processed...\n",
      ">> 138000 number of packets has been processed...\n",
      ">> 139000 number of packets has been processed...\n",
      ">> 140000 number of packets has been processed...\n",
      ">> 141000 number of packets has been processed...\n",
      ">> 142000 number of packets has been processed...\n",
      ">> 143000 number of packets has been processed...\n",
      ">> 144000 number of packets has been processed...\n",
      ">> 145000 number of packets has been processed...\n",
      ">> 146000 number of packets has been processed...\n",
      ">> 147000 number of packets has been processed...\n",
      ">> 148000 number of packets has been processed...\n",
      ">> 149000 number of packets has been processed...\n",
      ">> 150000 number of packets has been processed...>> Extracting features of 61 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 87 number of flows...\n",
      ">> Extracting features of 69 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 63 number of flows...\n",
      ">> Extracting features of 63 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 73 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 62 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 100 number of flows...\n",
      ">> Extracting features of 70 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 61 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 78 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 72 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 65 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 66 number of flows...\n",
      ">> Extracting features of 69 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 66 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 61 number of flows...\n",
      ">> Extracting features of 69 number of flows...\n",
      ">> Extracting features of 82 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 61 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 60 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 62 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 69 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 63 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 85 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 67 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 61 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 76 number of flows...\n",
      ">> Extracting features of 67 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 67 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 52 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 51 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 63 number of flows...\n",
      ">> Extracting features of 67 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> Extracting features of 57 number of flows...\n",
      ">> 151000 number of packets has been processed...\n",
      ">> 152000 number of packets has been processed...\n",
      ">> 153000 number of packets has been processed...\n",
      ">> 154000 number of packets has been processed...\n",
      ">> 155000 number of packets has been processed...\n",
      ">> 156000 number of packets has been processed...\n",
      ">> 157000 number of packets has been processed...\n",
      ">> 158000 number of packets has been processed...\n",
      ">> 159000 number of packets has been processed...\n",
      ">> 160000 number of packets has been processed...\n",
      ">> 161000 number of packets has been processed...\n",
      ">> 162000 number of packets has been processed...\n",
      ">> 163000 number of packets has been processed...\n",
      ">> 164000 number of packets has been processed...\n",
      ">> 165000 number of packets has been processed...\n",
      ">> 166000 number of packets has been processed...\n",
      ">> 167000 number of packets has been processed...\n",
      ">> 168000 number of packets has been processed...\n",
      ">> 169000 number of packets has been processed...\n",
      ">> 170000 number of packets has been processed...\n",
      ">> 171000 number of packets has been processed...\n",
      ">> 172000 number of packets has been processed...\n",
      ">> 173000 number of packets has been processed...\n",
      ">> 174000 number of packets has been processed...\n",
      ">> 175000 number of packets has been processed...\n",
      ">> 176000 number of packets has been processed...\n",
      ">> 177000 number of packets has been processed...\n",
      ">> 178000 number of packets has been processed...\n",
      ">> 179000 number of packets has been processed...\n",
      ">> 180000 number of packets has been processed...\n",
      ">> 182000 number of packets has been processed...\n",
      ">> 183000 number of packets has been processed...\n",
      ">> 184000 number of packets has been processed...\n",
      ">> 186000 number of packets has been processed...\n",
      ">> 187000 number of packets has been processed...\n",
      ">> 188000 number of packets has been processed...\n",
      ">> 189000 number of packets has been processed...\n",
      ">> 190000 number of packets has been processed...\n",
      ">> 191000 number of packets has been processed...\n",
      ">> 192000 number of packets has been processed...\n",
      ">> 193000 number of packets has been processed...\n",
      ">> 194000 number of packets has been processed...\n",
      ">> 195000 number of packets has been processed...\n",
      ">> 196000 number of packets has been processed...\n",
      ">> 197000 number of packets has been processed...\n",
      ">> 198000 number of packets has been processed...\n",
      ">> 199000 number of packets has been processed...\n",
      ">> 200000 number of packets has been processed...\n",
      ">> 201000 number of packets has been processed...\n",
      ">> 202000 number of packets has been processed...\n",
      ">> 203000 number of packets has been processed...\n",
      ">> 204000 number of packets has been processed...\n",
      ">> 205000 number of packets has been processed...\n",
      ">> 206000 number of packets has been processed...\n",
      ">> 207000 number of packets has been processed...\n",
      ">> 208000 number of packets has been processed...\n",
      ">> 209000 number of packets has been processed...\n",
      ">> 210000 number of packets has been processed...\n",
      ">> 212000 number of packets has been processed...\n",
      ">> 213000 number of packets has been processed...\n",
      ">> 214000 number of packets has been processed...\n",
      ">> 215000 number of packets has been processed...\n",
      ">> 216000 number of packets has been processed...\n",
      ">> 217000 number of packets has been processed...\n",
      ">> 218000 number of packets has been processed...\n",
      ">> 219000 number of packets has been processed...\n",
      ">> 220000 number of packets has been processed...\n",
      ">> 221000 number of packets has been processed...\n",
      ">> 222000 number of packets has been processed...\n",
      ">> 223000 number of packets has been processed...\n",
      ">> 224000 number of packets has been processed...\n",
      ">> 225000 number of packets has been processed...\n",
      ">> 226000 number of packets has been processed...\n",
      ">> 227000 number of packets has been processed...\n",
      ">> 228000 number of packets has been processed...\n",
      ">> 229000 number of packets has been processed...\n",
      ">> 230000 number of packets has been processed...\n",
      ">> 231000 number of packets has been processed...\n",
      ">> 232000 number of packets has been processed...\n",
      ">> 233000 number of packets has been processed...\n",
      ">> 234000 number of packets has been processed...\n",
      ">> 235000 number of packets has been processed...\n",
      ">> 236000 number of packets has been processed...\n",
      ">> 237000 number of packets has been processed...\n",
      ">> 238000 number of packets has been processed...\n",
      ">> 239000 number of packets has been processed...\n",
      ">> 240000 number of packets has been processed...\n",
      ">> 241000 number of packets has been processed...\n",
      ">> 242000 number of packets has been processed...\n",
      ">> 243000 number of packets has been processed...\n",
      ">> 244000 number of packets has been processed...\n",
      ">> 245000 number of packets has been processed...\n",
      ">> 246000 number of packets has been processed...\n",
      ">> 247000 number of packets has been processed...\n",
      ">> 248000 number of packets has been processed...\n",
      ">> 249000 number of packets has been processed...\n",
      ">> 250000 number of packets has been processed...\n",
      ">> 251000 number of packets has been processed...\n",
      ">> 252000 number of packets has been processed...\n",
      ">> 253000 number of packets has been processed...\n",
      ">> 254000 number of packets has been processed...\n",
      ">> 255000 number of packets has been processed...\n",
      ">> 256000 number of packets has been processed...\n",
      ">> 257000 number of packets has been processed...\n",
      ">> 258000 number of packets has been processed...\n",
      ">> 259000 number of packets has been processed...\n",
      ">> 260000 number of packets has been processed...\n",
      ">> 261000 number of packets has been processed...\n",
      ">> 262000 number of packets has been processed...\n",
      ">> 263000 number of packets has been processed...\n",
      ">> 264000 number of packets has been processed...\n",
      ">> 265000 number of packets has been processed...\n",
      ">> 266000 number of packets has been processed...\n",
      ">> 267000 number of packets has been processed...\n",
      ">> 268000 number of packets has been processed...\n",
      ">> 269000 number of packets has been processed...\n",
      ">> 270000 number of packets has been processed...\n",
      ">> 271000 number of packets has been processed...\n",
      ">> 272000 number of packets has been processed...\n",
      ">> 273000 number of packets has been processed...\n",
      ">> 274000 number of packets has been processed...\n",
      ">> 275000 number of packets has been processed...\n",
      ">> 276000 number of packets has been processed...\n",
      ">> 277000 number of packets has been processed...\n",
      ">> 278000 number of packets has been processed...\n",
      ">> 279000 number of packets has been processed...\n",
      ">> 280000 number of packets has been processed...\n",
      ">> 281000 number of packets has been processed...\n",
      ">> 282000 number of packets has been processed...\n",
      ">> 283000 number of packets has been processed...\n",
      ">> 284000 number of packets has been processed...\n",
      ">> 285000 number of packets has been processed...\n",
      ">> 286000 number of packets has been processed...\n",
      ">> 287000 number of packets has been processed...\n",
      ">> 288000 number of packets has been processed...\n",
      ">> 289000 number of packets has been processed...\n",
      ">> 290000 number of packets has been processed...\n",
      ">> 291000 number of packets has been processed...\n",
      ">> 292000 number of packets has been processed...\n",
      ">> 293000 number of packets has been processed...\n",
      ">> 294000 number of packets has been processed...\n",
      ">> 295000 number of packets has been processed...\n",
      ">> 296000 number of packets has been processed...\n",
      ">> 297000 number of packets has been processed...\n",
      ">> 298000 number of packets has been processed...\n",
      ">> 299000 number of packets has been processed...\n",
      ">> 300000 number of packets has been processed...\n",
      ">> 301000 number of packets has been processed...\n",
      ">> 302000 number of packets has been processed...\n",
      ">> 303000 number of packets has been processed...\n",
      ">> 304000 number of packets has been processed...\n",
      ">> 305000 number of packets has been processed...\n",
      ">> 306000 number of packets has been processed...\n",
      ">> 307000 number of packets has been processed...\n",
      ">> 308000 number of packets has been processed...\n",
      ">> 309000 number of packets has been processed...\n",
      ">> 310000 number of packets has been processed...\n",
      ">> 311000 number of packets has been processed...\n",
      ">> 312000 number of packets has been processed...\n",
      ">> 313000 number of packets has been processed...\n",
      ">> 314000 number of packets has been processed...\n",
      ">> Writing 60 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Writing 73 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 62 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 100 flows with extracted features...\n",
      ">> Writing 70 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Writing 61 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 78 flows with extracted features...\n",
      ">> Writing 72 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 115 flows with extracted features...\n",
      ">> Writing 115 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 60 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 65 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 66 flows with extracted features...\n",
      ">> Writing 69 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 66 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 61 flows with extracted features...\n",
      ">> Writing 69 flows with extracted features...\n",
      ">> Writing 82 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 61 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 113 flows with extracted features...\n",
      ">> Writing 104 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 62 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 69 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 63 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 85 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 67 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 61 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 76 flows with extracted features...\n",
      ">> Writing 67 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 67 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 52 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 51 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 63 flows with extracted features...\n",
      ">> Writing 67 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 56 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 57 flows with extracted features...\n",
      ">> Writing 53 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 62 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 64 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 55 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 68 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 62 flows with extracted features...\n",
      ">> Writing 62 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 59 flows with extracted features...\n",
      ">> Writing 54 flows with extracted features...\n",
      ">> Writing 50 flows with extracted features...\n",
      ">> Writing 82 flows with extracted features...\n",
      ">> 315000 number of packets has been processed...\n",
      ">> 316000 number of packets has been processed...\n",
      ">> 317000 number of packets has been processed...\n",
      ">> End of parsing pcap file(s).\n",
      ">>> 317430 packets analyzed and 20078 flows created in total.\n",
      "##################################################\n",
      ">> Parser Report:\n",
      "##################################################\n",
      ">>> Number and percentage of IP packets:\n",
      "      Total IP packets: 314434\n",
      "      Percentage of IP packets: 99.06%\n",
      "\n",
      ">>> Number and percentage of TCP packets:\n",
      "      Total TCP packets: 301499\n",
      "      Percentage of TCP packets: 94.98%\n",
      "\n",
      ">>> Number and percentage of UDP packets:\n",
      "      Total UDP packets: 12825\n",
      "      Percentage of UDP packets: 4.04%\n",
      "\n",
      "##################################################\n",
      ">> Preparing the output file...\n",
      "\n",
      ">> Writing 80 flows with extracted features...\n",
      ">> Writing 109 flows with extracted features...\n",
      ">> Writing 58 flows with extracted features...\n",
      ">> Extracting finished, lets go for final writing...\n",
      ">> Writing the last 0 flows with extracted features...\n",
      ">> Writing finished, lets wrapp up!\n",
      "\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 62 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 64 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 55 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 68 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 62 number of flows...\n",
      ">> Extracting features of 62 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 59 number of flows...\n",
      ">> Extracting features of 54 number of flows...\n",
      ">> Extracting features of 82 number of flows...\n",
      ">> Extracting features of 50 number of flows...\n",
      ">> Extracting features of 80 number of flows...\n",
      ">> Extracting features of 53 number of flows...\n",
      ">> Extracting features of 56 number of flows...\n",
      ">> Extracting features of 58 number of flows...\n",
      ">> Results are ready!\n"
     ]
    }
   ],
   "source": [
    "!python -m NTLFlowLyzer -c run_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "895dbb76-0eb4-4ea3-8705-094178441dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ded7494-b689-4c6e-9cf8-ccc50f006e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.read_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a608bc1-a4b4-437c-ab3c-21289a46e4b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6796 entries, 0 to 6795\n",
      "Data columns (total 28 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   flow_id                     6796 non-null   object \n",
      " 1   timestamp                   6796 non-null   object \n",
      " 2   src_ip                      6796 non-null   object \n",
      " 3   src_port                    6796 non-null   int64  \n",
      " 4   dst_ip                      6796 non-null   object \n",
      " 5   dst_port                    6796 non-null   int64  \n",
      " 6   protocol                    6796 non-null   object \n",
      " 7   total_payload_bytes         6796 non-null   int64  \n",
      " 8   fwd_total_payload_bytes     6796 non-null   int64  \n",
      " 9   bwd_total_payload_bytes     6796 non-null   int64  \n",
      " 10  payload_bytes_max           6796 non-null   int64  \n",
      " 11  payload_bytes_min           6796 non-null   int64  \n",
      " 12  payload_bytes_mean          6796 non-null   float64\n",
      " 13  payload_bytes_std           6796 non-null   float64\n",
      " 14  fwd_payload_bytes_max       6796 non-null   int64  \n",
      " 15  fwd_payload_bytes_mean      6796 non-null   float64\n",
      " 16  fwd_payload_bytes_std       6796 non-null   float64\n",
      " 17  fwd_payload_bytes_variance  6796 non-null   float64\n",
      " 18  bwd_payload_bytes_max       6796 non-null   int64  \n",
      " 19  bwd_payload_bytes_mean      6796 non-null   float64\n",
      " 20  bwd_payload_bytes_std       6796 non-null   float64\n",
      " 21  bwd_payload_bytes_variance  6796 non-null   float64\n",
      " 22  fwd_segment_size_mean       6796 non-null   float64\n",
      " 23  bwd_segment_size_mean       6796 non-null   float64\n",
      " 24  segment_size_mean           6796 non-null   float64\n",
      " 25  subflow_fwd_bytes           6796 non-null   float64\n",
      " 26  subflow_bwd_bytes           6796 non-null   float64\n",
      " 27  label                       6796 non-null   object \n",
      "dtypes: float64(13), int64(9), object(6)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "output.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e57ca1-215d-4d49-81c0-61a79ce14133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6abcac9c-5792-4168-964b-37c4b5640b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# 1. Modelini YÃ¼kle (Daha Ã¶nce eÄŸittiÄŸin .pkl dosyasÄ±)\n",
    "# Bu modelin iÃ§inde RandomForest veya XGBoost var varsayÄ±yoruz.\n",
    "model = joblib.load('random_forest_model1.joblib')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def apply_column_mapping(df):\n",
    "    # Senin saÄŸladÄ±ÄŸÄ±n dÃ¶nÃ¼ÅŸÃ¼m haritasÄ±\n",
    "    mapping = {\n",
    "        'fwd_total_payload_bytes': 'FwdTotalPayloadBytes',\n",
    "        'fwd_avg_segment_size': 'FwdSegmentSizeMean',\n",
    "        'total_payload_bytes': 'TotalPayloadBytes',\n",
    "        'bwd_total_payload_bytes': 'BwdTotalPayloadBytes',\n",
    "        'bwd_avg_segment_size': 'BwdSegmentSizeMean',\n",
    "        'bwd_payload_bytes_variance': 'BwdPayloadBytesVariance',\n",
    "        'avg_segment_size': 'SegmentSizeMean',\n",
    "        'fwd_payload_bytes_mean': 'FwdPayloadBytesMean',\n",
    "        'payload_bytes_variance': 'PayloadBytesVariance',\n",
    "        'bwd_payload_bytes_mean': 'BwdPayloadBytesMean',\n",
    "        'fwd_payload_bytes_std': 'FwdPayloadBytesStd',\n",
    "        'bwd_payload_bytes_std': 'BwdPayloadBytesStd',\n",
    "        'payload_bytes_mean': 'PayloadBytesMean',\n",
    "        'payload_bytes_std': 'PayloadBytesStd',\n",
    "        'fwd_payload_bytes_variance': 'FwdPayloadBytesVariance',\n",
    "        'fwd_payload_bytes_max': 'FwdPayloadBytesMax',\n",
    "        'bwd_payload_bytes_max': 'BwdPayloadBytesMax',\n",
    "        'payload_bytes_max': 'PayloadBytesMax',\n",
    "        'subflow_fwd_bytes': 'SubflowFwdBytes',\n",
    "        'subflow_bwd_bytes': 'SubflowBwdBytes'\n",
    "    }\n",
    "\n",
    "    # 1. SÃ¼tun isimlerini deÄŸiÅŸtir\n",
    "    # (inplace=False yaparak orijinal df'i bozmadan yeni bir df dÃ¶ndÃ¼rÃ¼yoruz)\n",
    "    df_renamed = df.rename(columns=mapping)\n",
    "\n",
    "    # 2. Modelin sadece bu sÃ¼tunlarÄ± bekliyorsa, diÄŸer gereksiz sÃ¼tunlarÄ± atabiliriz.\n",
    "    # AÅŸaÄŸÄ±daki kod sadece listedeki sÃ¼tunlarÄ± seÃ§er ve sÄ±ralamayÄ± garanti eder.\n",
    "    target_columns = list(mapping.values())\n",
    "    \n",
    "    # EÄŸer veride eksik sÃ¼tun varsa hata vermemesi iÃ§in kontrol:\n",
    "    available_columns = [col for col in target_columns if col in df_renamed.columns]\n",
    "    \n",
    "    return df_renamed[available_columns]\n",
    "\n",
    "# --- KULLANIM Ã–RNEÄžÄ° ---\n",
    "# df_ntflow = pd.read_csv(\"ntflow_ciktisi.csv\")\n",
    "# df_ready = apply_column_mapping(df_ntflow)\n",
    "\n",
    "# Kontrol etmek iÃ§in:\n",
    "# print(df_ready.columns)\n",
    "MODEL_FEATURES = [\n",
    "    'fwd_total_payload_bytes',\n",
    "    'fwd_avg_segment_size',\n",
    "    'total_payload_bytes',\n",
    "    'bwd_total_payload_bytes',\n",
    "    'bwd_avg_segment_size',\n",
    "    'bwd_payload_bytes_variance',\n",
    "    'avg_segment_size',\n",
    "    'fwd_payload_bytes_mean',\n",
    "    'payload_bytes_variance',\n",
    "    'bwd_payload_bytes_mean',\n",
    "    'fwd_payload_bytes_std',\n",
    "    'bwd_payload_bytes_std',\n",
    "    'payload_bytes_mean',\n",
    "    'payload_bytes_std',\n",
    "    'fwd_payload_bytes_variance',\n",
    "    'fwd_payload_bytes_max',\n",
    "    'bwd_payload_bytes_max',\n",
    "    'payload_bytes_max',\n",
    "    'subflow_fwd_bytes',\n",
    "    'subflow_bwd_bytes'\n",
    "]\n",
    "\n",
    "def make_prediction(df_raw):\n",
    "    \"\"\"\n",
    "    df_raw: NTLFlowLyzer'dan gelen ham DataFrame (Ä°Ã§inde label, ip, port her ÅŸey var)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ADIM 1: Veriyi HazÄ±rla (Preprocessing)\n",
    "    # Model sadece sayÄ±sal olan 20 feature'Ä± ister. \n",
    "    # 'label', 'src_ip', 'flow_id' gibi ÅŸeyleri modele veremeyiz.\n",
    "    \n",
    "    # Sadece ilgili 20 kolonu seÃ§ip yeni bir DataFrame (X) oluÅŸturuyoruz.\n",
    "    # EÄŸer kolon eksikse (NTL vermediyse) hata vermesin diye reindex kullanÄ±yoruz.\n",
    "    X = df_raw.reindex(columns=MODEL_FEATURES)\n",
    "    \n",
    "    # ADIM 2: Temizlik (BoÅŸluklarÄ± Doldur)\n",
    "    # NTLFlowLyzer hesaplayamadÄ±ÄŸÄ± yere boÅŸluk koyabilir, model NaN kabul etmez.\n",
    "    # AÄŸ trafiÄŸinde boÅŸluk genelde 0 demektir.\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # ADIM 3: Tip DÃ¶nÃ¼ÅŸÃ¼mÃ¼\n",
    "    # Her ÅŸeyin sayÄ± (float/int) olduÄŸundan emin ol.\n",
    "    X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    \n",
    "\n",
    "    X=apply_column_mapping(X)\n",
    "    \n",
    "    try:\n",
    "        predictions = model.predict(X)          # Ã‡Ä±ktÄ±: ['DDoS', 'Benign', ...]\n",
    "        probabilities = model.predict_proba(X)  # Ã‡Ä±ktÄ±: [[0.1, 0.9], ...] (GÃ¼ven oranÄ±)\n",
    "        \n",
    "        # GÃ¼ven skorunu (Confidence) al (En yÃ¼ksek olasÄ±lÄ±k)\n",
    "        confidence_scores = np.max(probabilities, axis=1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Tahmin hatasÄ±: {e}\")\n",
    "        return None\n",
    "\n",
    "    # ADIM 5: Sonucu Raporla (Metadata ile BirleÅŸtir)\n",
    "    # Tahmin sonucunu, orijinal verideki IP ve ID ile yan yana koyuyoruz.\n",
    "    results = df_raw[['src_ip', 'dst_ip', 'dst_port', 'flow_id']].copy()\n",
    "    results['prediction'] = predictions\n",
    "    results['confidence'] = confidence_scores\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Test Senaryosu ---\n",
    "\n",
    "# NTLFlowLyzer'dan gelen veri (Senin Ã¶rneÄŸindeki gibi, label 'Unknown' olabilir)\n",
    "# Bu veriyi CSV'den okuduÄŸunu veya Kafka'dan aldÄ±ÄŸÄ±nÄ± varsay.\n",
    "# df_incoming = pd.read_csv('ntl_output.csv') \n",
    "\n",
    "# Fonksiyonu Ã§aÄŸÄ±r\n",
    "# final_report = make_prediction(df_incoming)\n",
    "\n",
    "# Sonucu gÃ¶r\n",
    "# print(final_report)\n",
    "# Ã‡Ä±ktÄ± ÅŸÃ¶yle gÃ¶rÃ¼nÃ¼r:\n",
    "#      src_ip       dst_ip    prediction  confidence\n",
    "# 0  192.168.1.5  10.0.0.1      DDoS        0.98\n",
    "# 1  192.168.1.6  10.0.0.1     Benign       0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5028e20f-081a-4f01-8d64-046e00f5a67e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_incoming = pd.read_csv('output3.csv') \n",
    "results3=final_report=make_prediction(df_incoming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "858db313-b2f2-4e23-8108-7cb0b7c2cd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               src_ip           dst_ip  dst_port  \\\n",
      "0        172.31.64.41  169.254.169.254        80   \n",
      "1        172.31.64.41  169.254.169.254        80   \n",
      "2        172.31.64.41     54.230.36.67       443   \n",
      "3        212.92.116.6     172.31.64.41      3389   \n",
      "4        172.31.64.41   72.167.239.239        80   \n",
      "...               ...              ...       ...   \n",
      "20073   186.93.119.33     172.31.64.41       445   \n",
      "20074   186.93.119.33     172.31.64.41       445   \n",
      "20075  186.93.115.185     172.31.64.41       445   \n",
      "20076   186.93.119.33     172.31.64.41       445   \n",
      "20077    172.31.64.41    52.219.84.170       443   \n",
      "\n",
      "                                                 flow_id  prediction  \\\n",
      "0      172.31.64.41_49186_169.254.169.254_80_TCP_2018...           0   \n",
      "1      172.31.64.41_49191_169.254.169.254_80_TCP_2018...           0   \n",
      "2      172.31.64.41_49197_54.230.36.67_443_TCP_2018-0...           0   \n",
      "3      212.92.116.6_52088_172.31.64.41_3389_TCP_2018-...           0   \n",
      "4      172.31.64.41_49199_72.167.239.239_80_TCP_2018-...           0   \n",
      "...                                                  ...         ...   \n",
      "20073  186.93.119.33_65183_172.31.64.41_445_TCP_2018-...           0   \n",
      "20074  186.93.119.33_65203_172.31.64.41_445_TCP_2018-...           0   \n",
      "20075  186.93.115.185_50343_172.31.64.41_445_TCP_2018...           0   \n",
      "20076  186.93.119.33_49652_172.31.64.41_445_TCP_2018-...           0   \n",
      "20077  172.31.64.41_51972_52.219.84.170_443_TCP_2018-...           0   \n",
      "\n",
      "       confidence  \n",
      "0        0.900000  \n",
      "1        0.900000  \n",
      "2        0.900000  \n",
      "3        0.890000  \n",
      "4        0.900000  \n",
      "...           ...  \n",
      "20073    0.999551  \n",
      "20074    0.900000  \n",
      "20075    0.890000  \n",
      "20076    0.890000  \n",
      "20077    0.840000  \n",
      "\n",
      "[20078 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37321f33-b88b-4782-8c9d-954eae0a6bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tahmin DaÄŸÄ±lÄ±mÄ± ---\n",
      "prediction\n",
      "0    20078\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Tahmin DaÄŸÄ±lÄ±mÄ± ---\")\n",
    "print(results3['prediction'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dac8b32-d16a-464b-8f5b-6a5d2699579a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flow_id', 'timestamp', 'src_ip', 'src_port', 'dst_ip', 'dst_port',\n",
       "       'protocol', 'total_payload_bytes', 'fwd_total_payload_bytes',\n",
       "       'bwd_total_payload_bytes', 'payload_bytes_max', 'payload_bytes_min',\n",
       "       'payload_bytes_mean', 'payload_bytes_std', 'fwd_payload_bytes_max',\n",
       "       'fwd_payload_bytes_mean', 'fwd_payload_bytes_std',\n",
       "       'fwd_payload_bytes_variance', 'bwd_payload_bytes_max',\n",
       "       'bwd_payload_bytes_mean', 'bwd_payload_bytes_std',\n",
       "       'bwd_payload_bytes_variance', 'fwd_segment_size_mean',\n",
       "       'bwd_segment_size_mean', 'segment_size_mean', 'subflow_fwd_bytes',\n",
       "       'subflow_bwd_bytes', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8cd66e-ff00-4473-8a13-c366368a6c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semih\\Desktop\\bulldozer\\NTLFlowLyzer\\NTLFlowLyzer\n"
     ]
    }
   ],
   "source": [
    "%cd NTLFlowLyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596754f4-f756-42d5-bfdd-b53a0f70fa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semih\\Desktop\\bulldozer\\NTLFlowLyzer\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7b021-ca52-4450-9903-24cb130e43c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
